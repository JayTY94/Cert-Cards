(in_dtCurrentSheet.rows(j).item(0).ToString <> "") and (in_dtCurrentSheet.rows(j).item(7).tostring.Replace("$", "").IsNumeric or in_dtCurrentSheet.rows(j).item(7).tostring.contains("$"))

<wnd app='excel.exe' cls='XLMAIN' title='123BotOutput.xls  -  Compatibility Mode - Excel' />
<uia automationid='123BotOutput.xls' cls='ExcelBookTabControl' name='123BotOutput' />
<uia automationid='SheetTab' name='{{strSheetName}}' role='Sheet Tab' />

List<string>(2)
{
  "H18",
  "J25"
}

The following cards


These cards got reassigned to the Verkada user Lost Cards last weekend when Eric and Jay were cleaning up the users and making sure that users had only one card assigned.
191
    Deactivated
152
    Deactivated
47
    Deactivated
10
    Deactivated

I'm looking into which account exactly they belonged to. Going to take more time.


List of the something something

List of the URLs to find the reports that need updates on them.

New Dictionary(of String, String)
{
{"Table HD_Ticket 2022",
"http://support.hemic.com/adminui/analysis_report_list.php?CATEGORY_ID=&SEARCH_SELECTION_TEXT=table+HD_Ticket+2022&SEARCH_SELECTION=table+HD_Ticket+2022"},
{"Table USER",
"http://support.hemic.com/adminui/analysis_report_list.php?CATEGORY_ID=&SEARCH_SELECTION_TEXT=table+USER&SEARCH_SELECTION=table+USER"},
{"Table HD_WORK 2022",
"http://support.hemic.com/adminui/analysis_report_list.php?CATEGORY_ID=Service+Desk&SEARCH_SELECTION_TEXT=table+HD_WORK+2022&SEARCH_SELECTION=table+HD_WORK+2022"}
}


Tips to make better workflows using UiPath

RPA is taking hte world by storm. It allows the companies to automate mundane taks so that their resources could focus on something more important. This is beneficial not only for the company but also allows the employee to get their hands dirty in other areas of the industry as well.

Tools such as uiPath not only...

Here are some of the tips that could help you make yoru workflows more efficient -

#1 Using Global variables.

If you have any experience in programming, then you know the importance of global variables. While creating workflows we usually pass all the required variables as arguments to different workflows. The flaw with this approach is that if there is a small change in arguments of any one of the workflow, then all other workflows where it was called have to be modified.

In order to introduce global variable functionality in our workflow, we can use the Getter Setter activity.

It's looking like that's not in modern... Which is NOT a good sign!!!!


https://docs.uipath.com/studio/docs/test-activities

Test Activities

The Test Activity context menu option of the Designer panel is used for running a test of the currently selected activity. When clicked, the Locals panel opens displaying the variables and arguments in scope.

Test Activity can be used in two ways:

    Add default values to properties and test

    Add arguments and/or properties to activity properties and use the Local panel to add values after clickign the Test Activity option.

Double-click on the value field of a variable or argument or click the edit icon in the Locals pane, and add a new value. Next, click Step Into to focus and execute the activity, and monitor the variable or argument's value in the Locals panel.

The same is available when clickign Continue, but the variables are not visible in the Locals Panel.

Please take into consideration that dynamic checks when variabels depend on other variables that are defined later are not supported.

Execution logs generated by the Test Activity action are visible in the Output panel. Exceptions in Studio can be bubbled up, which means that the exception may be passed to parent containers in case it may be handled by them.

The Test Activity option is not available during debugging.

Example of Using Test Activity

For the If Activity, we created a process that asks the user for two numbers, checks to see if one is divisible by the otgher, and depending on the result, displays a different message in the Output panel.

To check the behavior of the If activity defined in the process, use the Test Activity option, as illustrated below:

The Test Activity action places the activity in the debugger and asks you for values to variables. Once provided, click the Continue button for the debugging process to continue. In this particular case, a message was written in the Output panel with the correct answer, which means that the expressions written in the If activity were correct.

Create Test Bench

The Create Test Bench option allows for the creation of automation building blocks, which can then be tested and added to the final workflow.

It is sued for testing activities, working with variables, and debugging the process. All this is done in a test bench workflow, a temporary sequence that's not part of the current proejct and that is discarded when closed.

The Create Test Bench option is similar to the Test Activity option, with the exception that the latter is contained and defined in an actual workflow.

To use the Create Test Bench option, go to the Activities panel search bar or use [the] Ctrl + Alt + F keyboard shortcut. Type the name of hte activity and right-click to open the context menu.

Select Create Test Bench[,] and the activity is automatically added to a sequence fiel not included in your project. From there yu can add other activities, chagne their default properties, and debug the process. The Output panel displays any logs or error found during debugging.

To save the file to your project, simplu use the ribbon option Save as, add a file name, and save it to the same file path as your project.

Please note that teh Create Test Bench does not work with the Pick Branch activity.

Run to this Activity

The Run to this Activity option is avialable when right-clicking an activity in the Designer panel.

This option starts the debugging process and pauses before teh selected activity is executed while highlighting it in the panel. If Run to this Activity is triggered when debugging is already paused, the execution continues until the activity is reached.

Run from this Activity

The Run from this Activity context menu option enters debugging in a paused state, allowing you to make changes to teh values of variables and arguments from the Locals Panel. Press Continue to start debugging or use actions such as Step Into, Step Over, Step Out.

Note:
An error occurs if you use Run from this Activity for an activity added inside one of the following container activities: Try Catch, Switch, Parallel, Pick, Trigger Scope, or Retry Scope.



https://docs.uipath.com/studio/docs/debugging-actions

Debugging Actions

Debugging of a single file or the whole project can be performed both form the Design or Debug ribbon tabs. However, the debugging process is not available if hte project files have validation errors.

Step Into

Use Step Into to debug activities one at a time. When this action is triggered, the debugger opens and highlights hte activity before it is executed.

When Step Into is used with Invoke Workflow File activities, the workflow is opened in a new tab in ReadOnly mode and each activity is executed one by one.

The keyboard shortcut for Step Into is F11.

Step Over

Unlike the Step Into action, Step Over does not open the current container. When used, the action debugs the next activity, highlighting containers (such as flowcharts, sequences or Invoke Workflow File activities) without opening thme.

This action comes in handy for skipping analysis of large containers which are unlikely to trigger any issues during execution.

Step Over is available using the F10 keybaord shortcut.

Step Out

As the name suggests, this action is used for stepping out and pausing the execution at the level of the current container. Step Out completes the execution activities of the current container before pausing the debugging. This option works well with nested sequences.

Step out is available using the Shift + F11 keyboard shortcut.

Retry

Retry re-executes the previous activity, and throws the exception if it's encountered again. The activity which threw the exception is highlighted and details about the error are shown in the Locals adn Call Stack Panels.

Ignore

The Ignore action can be used to ignore an encountered exception and continue the execution from the next activity so that the rest of the workflow can be debugged.

This action is useful when jumping over the activity that threw the exception and continuing debugging the remaining part of the project.

Restart

Restart is available after an exception was thrown and the debug process is paused. The action is used after restarting the debugging process from the first activity of the project. Use Slow Step to slow down the debugging speed and properly inspect activities as they are executed.

Please take into consideration that when using this option after using hte Run from this Activity action, the debugging is restarted from the previously indicated activity.

Break

Break allows you to pause the debugging process at any given moment. The activity which is being debugged remains highlighted when paused. Once this happens, you can choose to Continue, Step Into, Step Over, or Stop the debugging process.

It is recommended to use Break along with Slow Step so that you know exactly when debugging needs to be paused.

An alternative to using Slow Step in this situation is to keep an eye on the Output panel an duse Break on the activity that is currently being debugged.

Focus

Focus Execution Point helps you return to the current breakpoint or the activity that caused an error during debugging. The Focus button is used after navigating through the process, as an easy way to return to the activity that caused the error and resumes the debugging process.

Alternatively, when debugging is paused because a breakpoint was reached, Focus can be used for returning to said breakpoint, after navigating through activities contained in the automation process.

A third case is when the debugging is paused either after usign Step Into or Step Over and then navigating through the process. In this case, Focus returns to teh activity that paused the debugging process.

From the Breakpoints context menu, you can select focus to highlight the activity with the breakpoint.

Slow Step

Slow Step enables you to take a closer look at any activity during debugging. While this action is enabled, activities are highlighted in the debugging process. Moreover, containers such as flowcharts, sequences, or Invoke Workflow File activities are opened. This is simiar to using Step Into, but without having to pause the debugging process.

Slow Step can be activated both before or during the debugging process. Activating teh action does not pause debugging.

Although called Slow Step, the action comes with 4 different speeds. The selected speed step runs the debugging process slower than the previous one. For example, debugging with Slow Step at 1x runs it the slowest, and fastest at 4x. In other words, the speed dictates how fast teh debugger jumps from one activity to the next.

Each time you click Slow Step the speed changes by one step. You can easily tell by the icon, which updates accordingly.

Execution Trail

The Execution Trail ribbon button is disabled by default. When enabled, it shows the exact execution path at debugging. As the process is executed, each activity is highlighted and marked in the Designer panel, showing you the execution as it happens.

    executed activities are marked and highlighted in green.

    activities that were not executed are not marked in any way.

    activities that threw an exception are marked and highlighted in red.

https://docs.uipath.com/activities/docs/retry-scope

Retry Scope

UiPath.Core.Activities.RetryScope

Retries the contained activities as long as the condition  is not met or an error is thrown.

Important! Due to internal changes, this activity will no longer be visible in the Favorites list when upgrading to v20.4 or newer, if it was added to the Favorites list with a 19.4 or older version of UiPath.System.Activities.

Properties
Options
    NumberofRetries - The number of times that teh sequence is to be retried.

    RetryInterval - Specifies the amount of time (in seconds) between each retry.

Common
    DisplayName - The display name of this activity
    ContinueOnError - Specifies if the automation should continue even when the activity throws an error. This field only supports Boolean values (True, False). The default value is False. As a result, if the field is blank and an error is thrown, the execution of the project stops. If the value is set to True, the execution of the project continues regardless of any error.

Note:
If this activity is included in Try Catch and the value of ContinueOnError is True, no error is caught when the project is executed.

Misc
Private - If selected, the values of variables and arguments are no longer logged at Verbose level.

The Retry Scope activity is used for catching and handling an error, which is why it's similar to the Try Catch one. The following workflow attempts to open the Notepad window 3 times and uses the condition set in the Retry Scope activity to stop the loop.

    1. Create a new sequence and add the Retry Scope activity.
    2. In the Properties panel, leave the default NumberOfRetries of 3 and the Retry Interval of 5. This means that we attempt to open the Notepad window 3 times and the interval between tries is 5 seconds.
    3. In the Action section, add an Assign activity.
    4. Create a GernericValue variable, named for example Random and add it to the To field of the Assign activity.
    5. Add the Now.Millisecond mod 5 value to the variable by adding it to the Value field of the Assign activity.
    6. Add an if activity and as a condition enter Random <> 0. This means that you check if your variable is different than 0.
    7. In the Then section of this activity (the condition above is true):
        Add a Message Box stating "Notepad Window failed to start.".
        Under the Message Box, add a Throw activity to throw an error.
        Type in New System.Exception("Notepad failed to start.") in the Exception field, under Properties.
    8. In the Else section of the If activity (the condition above is false):
        Add an Open Application activity and indicate Notepad on the screen. Provide the full path of the Notepad executable file in the FileName field part of Properties.
    9. To exit the loop, add an Element Exists activity in the Condition section of Retry Scope and indicate the Notepad window.

This workflow simulates a failing Notepad window. If the value of the Random variable is 0 three times in a row, the "Notepad Window Failed to start" message is displayed every time and the entire workflow fails with the "Notepad failed to start" error. The latter message is the one added in the Throw activity.

If the value of the Random variable is 0, the Robot opens Notepad and because the exist condition of this loop is to find the Notepad window, teh workflow is successfully completed.

"<uia automationid='Grid' name='Grid' role='data grid' />
<uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' tableCol='39' tableRow='1' />"



TargetAnchorable { Accuracy=0.8, Anchor0=null, Anchor1=null, Anchor2=null, Anchor3=null, AnchorCount=0, Anchors=null, BrowserURL=null, CheckVisibility=false, ContentHash=null, DesignTimeRectangle=[{X=2414,Y=235,Width=64,Height=17}], ElementType=Cell, FriendlyName="'https://hemic365-my.shaâ€¦'", FullSelector="<wnd app='excel.exe' cls='XLMAIN' title='*.xlsx - Excel' /><uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' tableCol='39' tableRow='1' />", FullSelectorArgument=InArgument<string> { ArgumentType=[System.String], Direction=In, EvaluationOrder=-1, Expression=[<uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' tableCol='39' tableRow='1' />], Expression=[<uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' tableCol='39' tableRow='1' />] }, FuzzyAccuracy=0.5, FuzzyPartialSelector="<uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' />", FuzzySelector="<wnd app='excel.exe' cls='XLMAIN' title='*.xlsx - Excel' /><uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' />", FuzzySelectorArgument=InArgument<string> { ArgumentType=[System.String], Direction=In, EvaluationOrder=-1, Expression=[<uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' />], Expression=[<uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' />] }, Guid="9025c745-569d-4cfc-9a2c-12075e244398", Id="fc575b1c-3dd6-4698-abac-09e580bc552d", ImageBase64="", InformativeScreenshot="16ea0291930b4545f79e00d365eab6a3.png", IsInApplicationCardSingleWindow=false, IsNativeTextCaseSensitive=false, IsOffsetPointEnabled=false, IsResponsive=false, NativeText=null, NativeTextArgument=null, OCRAccuracy=0.7, OCRText=null, OwnerTarget=TargetAnchorable { ... }, OwnerTarget=TargetAnchorable { ... }, PartialSelector="<uia automationid='Grid' name='Grid' role='data grid' /><uia automationid='BA1' name='&quot;B&quot; &quot;A&quot; 1' role='item' tableCol='39' tableRow='1' />", PointOffset=null, PositioningType=Default, Reference=null, ScopeSelector="<wnd app='excel.exe' cls='XLMAIN' title='*.xlsx - Excel' />", ScopeSelector="<wnd app='excel.exe' cls='XLMAIN' title='*.xlsx - Excel' />", ScopeSelectorArgument=InArgument<string> { ArgumentType=[System.String], Direction=In, EvaluationOrder=-1, Expression=[<wnd app='excel.exe' cls='XLMAIN' title='*.xlsx - Excel' />], Expression=[<wnd app='excel.exe' cls='XLMAIN' title='*.xlsx - Excel' />] }, SearchSteps=Selector, SelectionStrategy=Default, TargetType=SelectorBased, TelemetryData=null, Text=null, TextMethod=None, TextSelector=null, WaitForReady=Interactive, WaitForReadyArgument=null }


https://hemic365-my.sharepoint.com/personal/jayoshimi_hemic_com/Documents/Documents/UiPath/UiPathAutomations/ReportFormatting/copyTEST%20H%20Path%20TWO.xlsx?web=1

https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-services-overview.html

AWS PrivateLink and VPC endpoints

AWS PrivateLink is a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services, services hosted by other AWS accounts (VPC endpoint services), and supported AWS Marketplace partner services. You do not need to use an internet gateway, NAT device, public IP address, AWS Direct Connection connection, or AWS Site-to-Site VPN connection to communicate with the service. Therefore, you control the specific API endpoints, sites, and services that are reachable from your VPC.

You can create your own VPC endpoint service, powered by AWS PrivateLink, and enable other AWS customers to access your service.

VPC endpoing concepts

The following are key concepts for VPC endpoints:

    VPC endpoint - The entry point in your VPC that enables you to connect privately to a service. The following are the different types of VPC endpoints. You create the type of VPC endpoint required by the supported service.

        Gateway Endpoint
        Interface Endpoint
        Gateway Load Balancer Endpoint

    Endpoint service - Your own application or service in your VPC. Other AWS principals can create an endpoint from their VPC to your endpoint service.

To use AWS PrivateLink, create a VPC endpoint for a service in your VPC. You create the type of VPC endpoing required by the supported service. This creates an elastic network interface in your subnet with a private IP address that serves as an entry point for traffic destined to securely connect your PVC to an AWS service that supports AWS PrivateLink.

Work with VPC Endpoints

You can create, acces, and manage VPC endpoints using any of the following:

    AWS Management Console - Provides a web interface that you can se to access your AWS PrivateLink resources.

    AWS Command Line Interfrace (AWS CLI) - Provides commands for a broad set of AWS services, including AWS PrivateLink. For more information about commands for AWS PrivateLink, see ec2 in the AWS CLI Command Reference.

    AWS CloudFormation - Create templates that describe your AWS resources. You can use teh templates to provision and manage those resources in a single unit. For more information, see the following AWS PrivateLink resources:
        AWS::EC2::VPCEndpoint
        AWS::EC2::VPCEndpointConnectionNotification
        AWS::EC2::VPCEndpointService
        AWS::EC2::VPCEndpointServicePermissions
        AWS::ELasticLoadBalancingV2::LoadBalancer

    AWS SDKs - Provide Language-specific APIs. The SDKs take care of many of the connection detils, such as calculating signatures, handling request retries, and handling errors. For more information, see AWS SDKs.

    Query API - Provides low-level API actions that you call using HTTPS requests. Using the Query API is the most direct way to access Amazon VPC. However, it requires that your application handle low-level details such as genearting hte hash to sign the request and handling errors. For more information, see AWS PrivateLink actions in teh Amazon EC2 API Refrence.

Example Endpoint Configurations

For information about AWS PrivateLink and VPC peering examples, see Examples: SErvices using AWS PrivateLink and VPC peering in the Amazon VPC User Guide.

Pricing for endpoints

For information about pricing, see AWS PrivateLink Pricing.

https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html

VPC Endpoints

A VPC endpoint enables connections between a virtual private cloud (VPC) and supported services, without requiring you use an internet gateway, NAT Device, VPN connection, or AWS DirectConnect connection. Therefore, you control the specific API endpoints, sites, and services that are reachable from your VPC.

VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. The following are the different types of VPC endpoints. You create the type of VPC endpoint that's required by the supported service.

Interface endpoints

An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet. It serves as an entry point for traffic destined to a service that is owned by AWS or owned by an AWS customer or partner. For a list of AWS services that integrate with AWS PrivateLink, see AWS services that integrate with AWS PrivateLink.

You are billed for hourly usage and data processing charges. For more information, see Interface Endpoint pricing.

Gateway Load Balancer endpoints

A Gateway Load Balancer endpoint is an elastic network interface with a private IP address from the IP address range of your subnet. It serves as an entry point to intercept traffic and route it to a network for security service that you've configured using a Gateway Load Balancer. You specify a Gateway Load Balancer endpoint as a target for a route in the route table. Gateway Load Balancer endpoints are supported only for endpoint services that are configured usin ga Gateway Load Balancer.

You are billed for hourly usage and data processing charges. For more information, see Gateway Load Balancer endpoint pricing.

Gateway endpoints

A gateway endpoint is a gateway that is a target for a route in your route table used for traffic destined to either Amazon S3 or DynamoDB.

There is no charge for using gateweay endpoints.

Amazon S3 supports both gateway endpoints and interface endpoints. For a comparison of the two options, see Types of VPC endpoints for Amazon S3 in the Amazon S3 User Guide.

https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html

An interface VPC endpoint (interface endpoint) allows you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and Partners in their own VPCs (referred to as endpoint services), and supported AWS Marketplace Partner services. The owner of the service is the service provider, and you, as the principal creating the interface endpoint, are the service consumer.

The following are the general steps for seting up an inteface endpoint:

    1. Choose the VPC in which to create the interface endpoint, and provide the name of the AWS service, endpoint service, or AWS Marketplace service to which you're connecting.

    2. Choose a subnet in your VPC to use the interface endpoint. We create an endpoint network interface in the subnet. An endpoint network interface is assigned a private IP address from the IP address range of your subnet, and keeps this IP address until the interface endpoint is deleted. You can specify more than one subnet in different Availability Zones (as supported by the service) to help ensure that your interface endpoint is resilient to Availability Zone failures. In that case, we create an endpoint network interface in each subnet you specify.

    Note.
    An endpoint network interface is a requester-managed network interface. You can view it in your account, but you cannot manage it yourself. For more information, see Requester-managed network interfaces.

    3. Specify the security groups to associate with the endpoint network interface. The security group reules control the traffic to the endpoint network interface from resources in your VPC. If you do not specify a security group, we associate teh default security group for the VPC.

    4. (Optional, AWS services and AWS Marketplace Partner services only) Enable private DNS for the endpoint so you can make requests to the service using its default DNS hostname.

    Important:
    Private DNS is turned on by default for endpoints created for AWS service and AWS Marketplace Partner services.
    Private DNS is turned on in the other subnets which are in the same VPC and Availability Zone or Local Zone.

    5. When the service provider and the consumer are in different accounts, see Interface endpoint Availability Zone considerations for information about how to use Availaibilty Zone IDs to identify the interface endpoint Availability Zone.

    6. After you create the interface endpoint, it's available to use when it's accepted by the service provider. The service provider must configure the service to accept requests automatically or manually. AWS services and AWS Marketplace services generally accept all endpoint requests automatically. For more information about the lifecycle of the endpoint, see Interface Endpoint lifecycle.

Services cannot initiate requests to requests in your VPC through the endpoint. An endpoint only returns responses to traffic that is initiated from resources in your VPC. Before you integrate a service and an endpoint, review the service-specific VPC endpoint documentation for any service-specific configuration and limitations.

...

Interface endpoint properties and limitations

To use interface endpoints, you need to be aware of their properties and current limitations:

    For each interface endpoint, you can choose only one subnet per availability zone.

    Services might not be available in all Availability Zones through an interface endpoint. To find out which Availability Zones are supported, use the describe-vpc-endpoint-services command or use the Amazon VPC console. For more information, see Create an interface endpoint.

    When you create an interface endpoint, the endpoint is created in the Availability Zone that is mapped to your account and that is independent from other accounts. When the service provider and the consumer are in different accounts, see Interface Endpoint Availability Zone considerations for information about how to use Availibility Zone IDs to identify the interface endpoint Availability Zone.

    When the service provider and the consumer have different accounts and us multiple Availability Zones, and the consumer views the VPC endpoint service information, the response only includes the common Availabilty Zones. For example, when the service provider account uses us-east-1a and us-east-1c and the consumer users us-east-1a and us-east-1b, the response includes the VPC endpoint services in teh common Availability Zone, us-east-1a.

    By default, each interface endpoint can support a bandwidth of up to 10 Gbps per Availability Zone and automatically scales up to 40 Gbps. If your application needs higher throughput per zone, contact AWS support.

    If the network ACXL for your subnet restricts traffic, you might be able to send traffic through the ndpoint netowrk interface. Ensure that you add appropriate rules that allow traffic to and from teh CIDR block of the subnet.


https://medium.com/javascript-scene/higher-order-functions-composing-software-5365cf2cbe99

Higher Order Functions (Composing Software)

Note: This is part of the "Composing Software" series (now a book!) on learning functional programming and compositional software techniques in JavaScriptES6+ from the ground  up. Stay tuned. There's a lot more of this to come!

A higher order function is a function that takes a function as an argument or returns a function. Higher order function is in contrast to first order functions, which don't take a function as an argument or return a function as an output.

Earlier, we saw examples of .map() and .filter(). Both of them take a function as an argument. They're both higher order functions.

Let's look at an example of a first-order function which filters all the 4-letter words from a list of words:

    const censor = words => {
        const filtered = [];
        for (let i=0, { length } = words; i < length; i++) {
            const word = words[i];
            if (word.length !==4) filtered.push(word);
        }
        return filtered;
    };

    censor(['oops', 'gasp', 'shout', 'sun']);
    // ['shout', 'sun']


Now, what if we want to select all the words that begin with 'S'? We could create another function:

    const startsWiths = words => {
        const filtered = [];
        for (let i=0, { length } = words; i < length; i ++) {
            const word = words[i];
            if (word.startsWith('s')) filtered.push(word) ;
        }
        return filtered;
    }

    startsWithS(['oops', 'gasp', 'shout', 'sun']);
    // ['shout', 'sun']

You may already be recognizing a lot of repeated code. There's a pattern forming here that could be abstracted into a more generalized solution. These two functions have a whole lot in common. They both iterate over a list and filter it on a given condition.

Both the iteration and the filtering seem like they're begging to be abstracted so they can be shared adn reused to build all sorts of similar functions. After all, selecting things form lists of things is a very common task.

Luckily for us, JavaScript has first class functions. What does that mean? Just like numbers, strings, or objects, functions can be:

    Assigned as an identifier (variable) value
    Assigned to object property values
    Passed as arguments
    Returned from functions

Basically, we can use functions just like any other bits of data in our programs, and that makes abstraction a lot easier. For instance, we can create a function that abstracts the process of iterating over a list and accumulating a return value by passing in a function that handles the bits that are different. We'll call that function reducer:

    const reduce = (reducer, initial, arr) => {
        //shared stuff
        let acc = initial;
        for (let i = 0, { length } = arr; i < length; i++) {

            //unique stuff in reducer() call
            acc = reducer(acc, arr[i]);

        //more shared stuff
        }
        return acc;
    };

    reduce((acc, curr)) => acc + cur, 0, [1,2,3]); // 6

This reduce() implementation takes a reducer function, an intial value for the accumulator, and an array of data to iterate over. For each item in hte array, the reducer is called, passing it the accumulator and the current arrray element. The return value


https://forum.uipath.com/t/whitelisting-requirements-for-uipath-studio-and-uipath-robot/307527
Whitelisting Requirements For UiPath Studio and UiPath Robot

What are the whitelisting requirements for UiPath Studio and UiPath Robot?

Reason and Intention: the purpose of this guide is to document the whitelisting requirements for UiPath Studio and UiPath Robot in desktop environments where there is strict enforcement of Group Policies that prevent UiPath Studio and UiPath Robot from functioning properly. This document aims to highlight the whitelisting requirements when the following restrictions are put in place:

    1. Prevention of any executable (.exe) file from being executed when the file is not residing in "C:\Program Files" or "C:\Program Files (x86)"
    2. Prevention of extensions from being loaded on Google Chrome and Microsoft Edge (Chromium)

The whitelisting is not something



https://docs.aws.amazon.com/whitepapers/latest/using-power-bi-with-aws-cloud/the-microsoft-power-bi-suite.html
The Microsoft Power BI Suite

To reduce confusion due to product naming similarities, this whitepaper presents what each Microsoft Power BI product and service is.

Power BI Desktop

Power BI Desktop is a free application you install on your local computer. It lets you connect to, transform, and visualize your data. With Power BI Desktop, you can connect to multiple different sources of data and combine them (often called modeling) into a data model. This data model lets you build visuals and collections of visuals you can share as reports inside your organization.

...

Power BI Service

Power BI is a collection of software services, apps, and connectors that work together to help you create, share, and consume business insights in a way that serves you and your business more effectively.

The Power BI Service is a cloud-based service. It supports light report editing and collaboration for teams and organizations. You can connect to data sources in teh Power BI service too, but modeling is limited.


https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html

What is AWS Control Tower?

AWS Control Tower offers a straightforward way to set up and govern an AWS multi-account environment, following precriptive best practices. AWS Control Tower orchestrates the capabilities of several other AWS services, including AWS Organizations, AWS Service Catalog, and AWS Single Sign-on, to build a landing zone in less than an hour. Resources are set up and managed on your behalf.

AWS Control Tower orchestration extends the capabilities of AWS Organizations. To help keep your organizations and accounts from drift, which is divergence from best practices, AWS Control Tower applies preventive and detective controls (guardrails). For example, you can use guardrails to help ensure that security logs and necessary cross-account access permissions are created, and not altered.

...

Features

AWS Control Tower has the following features:

    Landing Zone - A landing zone is a well architectured, multi-account environment that's based on security and compliance best practices. It is the enterprise-wise container that hold all of your organizational units (OUs), accouts, users, and other resources that you want to be subject to compliance regulation. A landing zone can scale to fit the needs of an enterprise of any size.

    Guardrails - A guardrail is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language. Two kinds of guardrails exist: preventative and detective. Three categories of guidance apply to the two kinds of guardrails: mandatory, strongly recommended, and elective. For more information about guardrails, see How Guardrails work.

    Account Factory - An Account Factory is a configurable account template that helps to standardize the provisioning of new accounts with pre-approved account configurations. AWS Control Tower offers a built-in Account Factory that helps automate the account provisioning workflow in your organization. For more information, see Provision and manage accounts with Account Factory.

    Dashboard - The dashboard offers continuous oversight of your landing zone to your team of central cloud administrators. Use the dashboard to see provisioned accounts across  your enterprise, guardrails enabled for policy enforcement, guardrails enabled for continuous detection of policy non-conformance , and noncompliant resources organized by accounts and OUs.

...

Configuration, Governance, and Extensibility

    Automated account configuration: AWS Control Tower automates account deployment and enrollment by means of an Account Factory (or "vending machine"), which is bult as an abstraction on top of provisioned products in AWS Service Catalog. The Account Factory can create and enroll AWS accounts, and it automates the process of applying guardrails and policies to those accounts.

    Centralized governance: By employing the capabilities the cappabilities of AWS Organizations, AWS Control Tower sets up a framework that ensures consistent compliance and governance across your multi-account environment. The AWS Organizations service provides essential capabilities for managing a multi-account environment, including central governance and management of accounts, creation of accounts from APIs, and service control policies (SCPs).

    Extensibility: You can build or extend your own AWS Control Tower environment by working directly in AWS Organizations, as well as in the AWS Control Tower console. You can see your changes reflected in AWS Control Tower after you register your existing organizations and enroll your existing accounts into AWS Control Tower. YOu can update your AWS Control Tower landing zone to reflect your changes. If your workloads require further advanced capabilities, you can leverage other AWS partner solutions along wiht AWS Control Tower

...

https://docs.aws.amazon.com/controltower/latest/userguide/how-control-tower-works.html
...

Structure of an AWS Control Tower Landing Zone

The structure of a landing zone in AWS Control Tower is as follows:

    Root--The parent that continas all other OUs in your landing zone.
    Security OU--This OU contians the Log Archive and Audit accounts. These accounts often are referred to as shared accounts. You can choose customized names for these shared accounts when you launch your landing zone. However, they cannot be renamed later.
    Sandbox OU--The Sandbox OU is created when you launch your landing zone, if you enable it. This and other registered OUs contain the enrolled accounts that your users work with to perform their AWS workloads.
    AWS SSO directory--This directory houses your AWS SSO users. It defines the scope of permissions for each AWS SSO user.
    AWS SSO users--These are the identifiers that yoru users can assume to perform their AWS workloads in your landing zone.


What happens when you set up a landing zone?

...

Safely Managing Resources Within your AWS Control Tower Landing Zone and Accounts

    When you create your landing zone, a number of AWS resources are created. To use AWS Control Tower, you must not modify or delete these AWS Control Tower managed resources outside of the supported methods described in this guide. Deleting or modifying these resources will cause your landing zone to enter an unknown state. For details, see Guidance for creating and modifying AWS Control Tower resources.

    When you enable optional guardrials (those with strongly recommended or elective guidance), AWS Control Tower creates AWS resources that it manages in your accounts. Do not modify or delete resources creatd by AWS Control Tower. Doing so can result in the guardrails entering an unknown state. For more information, see Guardrail reference.

What are the Shared Accounts?

In AWS Control Tower, three shared accounts in your landing zone are provisioned automatically during setup: the management account, the log archive account, and the audit account.

What is the management account?

This is the account that you created specifically for your landing zone. This account is used for billing for everything in your landing zone. It's also used for Account Factory provisioning for accounts, as well as to manage OUs and guardrails.

Note: It is not recommended to run any type of production workloads from an AWS ControlTower management account. Creating a separate AWS Control Tower account to run your workloads.

When you set up your landing zone, the following AWS resources are created within your management account.

AWS service | Resource type | Resource name
AWS Organizations | Accounts | audit, log archive
AWS Organizations | OUs | Security, Sandbox
AWS organizations
...

What is the log archive account?

This account works as a repository for logs of the API activities and resource configurations from all accounts in the landing zone.

When you set up your landing zone, the following AWS resources are created within your log archive account.

...


What is the audit account?

The audit account is a restricted account that's designed to give your security and compliance teams read and write access to all accounts in your landing zone. From the audit account, you have programmatic access to review accounts, by means of a role that is granted to Lambda functions only. The audit account does not allow you to log in to other accounts manually. For more information about Lambda functiosn and roles, see Configure a Lambda function to assume a role from another AWS account.

When you set up your landing zone, the following AWS resources are created within your audit account.

...


Detective guardrails detect specific events when they occur and log the action in CloudTrail. For example, the strongly recommended guardrail called Detect Whether Encryption is Enabled for Amazon EBS Volumes Attached to Amazon EC2 Instances detects whether an unencrypted Amazon EBS volume is attached to an EC2 instance in your landing zone.

For those who are familiar with AWS: In AWS Control Tower preventative guardrails are implemented with Service Control Policies (SCPs). Detective guardrails are implemented with AWS Config rules.

Related Topics:
    Guardrails in AWS Control Tower
    Detect and resolve drift in AWS Control Tower

How AWS Control Tower Works With StackSets

AWS Control Tower uses AWS CloudFormation StackSets to set up resources in your accounts. Each Stack Set has StackInstances that correspond to accounts, and to AWS Regions per account. AWS Control Tower deploys one stack set per instance per account per Region.

AWS Control Tower applies updates to certain accounts and AWS Regions selectively, absed on CloudFormation parameters. When updates are applied to some stack instances, other stack instances may be left in Outdated status. This behavior is expected and normal.

When a stack instance goes into Outdated status, it usually means that the stack corresponding to that stack instance is not aligned with the latest template in the stack set. The stack remains in the older template, so it might not include the latest resources or parameters. THe stack is still completely useable.

Here's a quick summary of what behavior to expect, based on AWS CloudFormation parameters there are specified during an update

If the stack set update includes changes to the template (that is, if the TemplateBody or TemplateURL properties are specified), or if the Parameters property is specified, AWS CloudFormation marks all stack instances with a status of Outdated prior to updating the stack instances in the specified accounts and AWS Regions. If the stack set update does not include changes to the template or parameters, AWS CloudFormation updates the stack instances in the specified accounts and Regions, while leaivng all other stack instances with their existing stack instance status. To update all of the stack instances associated with a stack set, do not specify the Accounts or Regions properties.

For more information, see Update your Stack Set in the AWS CloudFormation User Guide.

...

https://docs.aws.amazon.com/controltower/latest/userguide/planning-your-deployment.html

Plan your AWS Control Tower landing zone

WHen you go through the setup process, AWS Control Tower launches a key resource associated with your accoung, called a landing zone, which serves as a home for your organizations and their accounts.

Note: You have one landing zone per organization.

For information about some best practices to follow when you plan and set up your landing zone, see AWS multi-account stragety for your AWS Contol Tower landing zone.

Ways to set up AWS Control Tower

You can set up AWS Control Tower landing zone in an existing organization, or you can start by creating a new organization that contains your AWS Control Tower landing zone.

    Launch AWS Contorl Tower in an Existing Organization:

[I don't think that this page is going to be too important because the landing zone is already set up; going to skim though.]

...

About Extending Governance: Extending governance applies to specific OUs and accounts within a single organization that's already registered with AWS Control Tower, which means that a landing zone already exists for that organization. Extending governance means that AWS Control Tower guardrails are extended so that their constraints apply to the specific OUs and accounts within that registered organization. In this case, you're not launching a new landing zone, you're only expanding teh current landing zone for your organization.

Important:
    Special consideration: If you currently are using the AWS Landing Zone solution (ALZ) for AWS Organizations, check with your AWS solutions architect before you try to enable AWS Control Tower for your organization. AWS Control Tower cannot perform pre-checks that determine wheter AWS Control Tower  may interfer with your current landing zone deployment. For more information, see Walkthrough: Move form ALZ to AWS Control Tower. Also, for information about moving accounts from one landing zone to anohter, see What if the account does not meet the prerequisites?

https://docs.aws.amazon.com/controltower/latest/userguide/terminology.html

Terminology

Here's a quick review of some terms you'll see in AWS Control Tower documentation.

First, it's good to know that AWS Control Tower shares a lot of terminology with the AWS Organizations service, includign the terms organization and Organizational Unit (OU), which appear throughout this document.

    For more information about organizations and OUs, see AWS Organizations terminology and concepts. If you're new to AWS Control Tower, that terminology is a good place to begin.

    AWS Organizations is an AWS service that helps you centrally govern your environment as you grow and scale your workloads on AWS. AWS Control Tower relies on AWS Organizations to create accounts, to enforce preventative guardrails at the OU level, and to provide centralized billing.

    An AWS Account Factory account is an AWS account provisioned using Account Factory in AWS Control Tower. Sometimes, Account Factory is referred to informally as a "vending machine" for accounts.

    Your AWS Control Tower home Region is the AWS Region in which your AWS Control Tower Landing zone was deployed. You can view your home Region in your landing zone settings.

    AWS Service Catalog allows you to manage commonly deployed IT services, centrally. In the context of this document, Account Facotry uses AWS Service Catalog to provision new AWS accounts.

    AWS CloudFormation StackSets are a type of resource that extends the functionality of stakcs so that you can create, update, or delete stakcs across multiple accounts and Regions with a single operation and a single CloudFormation template.

    A stack instance is a reference to a stack in a target account within a Region.

    A stack is a collection of AWS resources that you can manage as a single unit.

    An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from multiple accounts and Regions within the organization, allowing you to view and query this compliance data within a single account.

    A conformance pack is a collection of AWS Config rules and remediation actions that can be deployed as a single entity in an account and a Region, or across an organization in AWS Organizations. You can use a conformance pack to help customize your AWS Control Tower environment. For technical blogs that provide more details, see Related Information.

    Baseline: To baseline an account is to set up its blueprints and guardrails. The baselining process also sets up the centralized logging and security audit roles on the account, as part of deploying the blueprints. AWS Control Tower baselines are contained in the roles that you apply to every enrolled account.

    Drift: A change in a resource installed by and configured by AWS Control Tower. Resources withoutd rive enable AWS Control Tower to function properly.

    Non-compliant resource: A resource that is in violation of an AWS Config rule that defines a particular detective guardrail.

    Shared account: One of the three accounts that AWS Control Tower creates automatically when you set up your landing zone: the management account, the log archive account, and the audit account. You can choose customized names for the log archive account and hte audit account during setup.

    Member account: A member account belongs to the AWS Control Tower organization. The member account can be enrolled or unenrolled in AWS Control Tower. WHen a registered OU contains a mix of enrolled and unenrolled accounts:

        Preventative guardrails enabled on the OU apply to all accounts within it, including unenrolled ones. This is true because preventative guardrails are enforced with SCPs at the OU level, not the account level. For more information, see Inheritance for service control plicies in the AWS Organizations documentation.

        Detective gueardrails enabled on the OU do not apply to unenrolled accounts.

    An account can be a member of only one organization at a time, and its charges are billed to the management account for that organization. A member account can be moved to the root container of an organization.

    AWS account: An AWS account acts as a resource container and resource isolation boundary. An AWS account can be associated with billing and payment. An AWS account is different than a user accou nt (sometimes called an IAM account) in AWS Control Tower. Accounts created through the Account Factory provisioning process are AWS accounts. AWS Accounts also can be added to AWS Control Tower by means of the account enrollment or OU registration process.

    Guardrail: A guardrail is a high-level rule that provides ongoing governance for your overall AWS Control Tower environment. Each guardrail enforces a single rule. Preventative guardrails are implemented with SCPs. Detective guardrails are implemented with AWS Config rules. For more information, see How Guardrails Work.

    Landing Zone: A landing zone is a cloud environment that offers a recommended starting point, including default accounts, account structure, network and security layouts, and so forth. From a landing zone, you can deploy workloads that utilize your solutions and applications.

    Nested OU: A nested OU in AWS Control Tower is an OU contained in another OU. A nested OU can have excactly one parent OU, and each account can be a member of exactly one OU. Nested OUs create a heirarchy. When you attach a policy to one of the OUs in the hierarchy, it flows down and affects all the OUs and accounts beneath it. A nested OU hierarchy in AWS Control Tower can be a maximum of five levels deep.

    Parent OU: The OU immediately above the current OU in the hierarchy. An OU can have many child OUs.

    Child OU: Any OU below the current OU in teh hierarchy. An OU can have many child OUs.

...

https://docs.aws.amazon.com/controltower/latest/userguide/best-practices.html

Best practices for AWS Control Tower

This topic is intended primarily for management account administrators.

Management account administrators are responsible for explaining some tasks that AWS Control Tower Guardrails prevent their member account administrators from doing. This topic describes some best practices and procedures for transfering this knowledge, and it gives other tips for setting up and maintaining your AWS Control Tower environment efficiently.

Explaining Access to Users

The AWS Control Tower console is available only to users with the management account administrator permissions. Only these users can perform administrative work within your landing zone. In accordance with best practices, this means that the majority of your users and member account administrators will never see the AWS Control Tower console. As a member of the management account administrator group, it's your responsibility to explain the following information to users anda dministrators of your member accounts, as appropriate.

    Explain which AWS resources users and administrators have acces to within the landing zone.
    List the preventative guardrails that apply to each Organizational Unit (OU) so that the other adminsitrators can plan and execute their AWS workloads accordingly.

Explaining Resource Access

Some administrators and other users may need an explanation of the AWS resources to which they have access within your landing zone. This access can include programmatic access and console-based access. Generally speaking, read access and write access for AWS resources is allowed. To perform work within AWS, your users require some level of access to specific services they need to do their jobs.

Some users, such as your AWS developers, may need to know about the resources to which they have access, so they can create engineering solutions. Other users, such as the end users of applications that run on AWS services, do not need to know about AWS resources in your landing zone.

AWS offers tools to identify the scope of a user's AWS resource access. After you identify the scope of a user's access, you can share that information with the user, in accordance with your organization's information management policies. For more information about these tools, see the links that follow.


    AWS access advisor - The AWS Identity and Access Management (IAM) access advisor tool lets you determine the permissions that your developers have by analyzing the last timestamp when an IAM entity, such as a user, role, or group, called an AWS service. Youc an audit service access and remove uneccessary permisions, and you can automate the process if needed. For more information, see our AWS Security blog post.

    IAM policy simulator - With the IAM policy simulator, you can test and troubleshoot IAM-based and resource-based policies. For more information, see Testing IAM Policies with the IAM Policy Simulator.

    AWS CloutTrail logs - You can review AWS CloudTrail logs to see actions taken by a user, role, or AWS service. For more information about CloudTrail, see the AWS CloudTrail User Guide.

    Actions taken by CloudTrail landing zone administrator are logged in the landing zone management account. Actions taken by member account administrators and users are logge din the shared log archive account.

    You can view a summary table of AWS Control Tower events in the Activities page.

Explaining Preventative Guardrails

A preventative guardrail ensures that your organization's accounts maintain compliance


https://docs.aws.amazon.com/controltower/latest/userguide/provision-and-manage-accounts.html

Methods of provisioning

AWS Control Tower provides four methods for creating and updating member accounts. Two methods are primarily console-based, and two methods are primarily automated.

Overview

The standard way to create member accounts is through Account Factory, a console-based product that's part of the AWS Service Catalog. If your landing zone is not in a state of drift, you can use Enroll account as a method to add new accounts form teh console, as well as to enroll existing AWS accounts into AWS ControlTower.

If you intend to set up or manage many accounts as a group, you may prefer an automated method: either to configure new accounts programmatically using IAM roles and Lambda functions, or to use a GitOps approach with AFT.

Console-based methods:
    Through the Account Factory console that is part of AWS Service Catalog. See Provision and mange accounts through Account Factory.
    From the AWS Control Tower Account Factory for Terraform (AFT), which relies on Account Factory and a GitOps model to allow automation of account provisioning and updating. See Provisiona ccounts with AWS Control Tower Account Factory for Terraform.

What happens when AWS Control Tower creates an account

New accounts in AWS Control Tower are created and then provisioned by an interaction among AWS Control Tower, AWS Organizations, and AWS Service Catalog. For steps to create an account through the AWS Control Tower see Create or Enroll An Individual Account.

Behind the scenes of account creation

    1. You initiate the request, for example, from the AWS Control Tower Account Factory page, or directly from the AWS Service Catalog console, or by calling the AWS Service Catalog ProvisionProductAPI.
    2. AWS Service Catalog calls Control Tower.
    3. AWS Control Tower begins a workflow, which as a first step calls the AWS Organizations CreateAccount API.
    4. After AWS Organizations creates the account, AWS Control Tower completes the provisioning process by applying blueprints and guardrails.
    5. AWS Service Catalog continues to poll AWS Control Tower to check for completion of the provisioning process.
    6. When the workflow in AWS Control Tower is complete, AWS Service Catalog finalizes the account's state and informs you (the requester) of the result.

https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html

AWS Organizations terminology and concepts

To help you get started with AWS Organizations, this topic explains some of the key concepts.

The following diagram shows a basic organization that consists of five accounts that are organized into four organizational units (OUs) under the root. The organization also has several policies that are attached to some of the OUs or directly to the accounts. For a description of each of tehse items, srefer to the definitions ni this topic.

Organization
An entity that you create to consolidate your AWS accounts so that you can administer them as a single unit. You can use the AWS Organizations console to centrally view and manage all of your accounts within your organization. An organization has one management account along with zero or more member accounts. You can organize the accounts in a hierarchical, tree-like structure with a root at the top and organizational units nested under the root. Each acount can be directly in the root or placed in one of the OUs in the hierarchy. An organization has the functionality that is determined by the feature set you enable.

Root
The parent container for all the accounts in your organization. If you apply a policy to the root, it applies to all organizational units (OUs) and accounts in the organization.

Note: Currently, you can have only one root. AWS Organizations automatically creates it for you when you create an organization.

Organizational Unit (OU)
A container for accounts within a root. An OU can also contain other OUs, enabling you to create a hierarchy that resembles an upside-down tree, with a root at the top and branches of OUs that reach down, ending in accounts that are the leaves of the tree. When you attach a policy to one of hte nodes in the hierarchy, it flows down and affects all the branches (OUs) and leaves (accounts) beneath it. An OU can have exactly one parent, and currently each account can be a member of exactly one OU.

Account
An account in Organizations is a standard AWS account that contains your AWS resoureces and the identities that can access those resources.

tip: An AWS account is not the same thing as a "user account". An AWS user is an identity that you create using AWS Identity and Access Management (IAM) and takes the form of either an IAM user with long-term credentials, or an IAM role with short-term credentials. A single AWS account can, and typeically does contain many users and roles.

There are two types of acconts in an organization: a signle account that is designated as the management account, and one or more member accounts.

    The management account is the account that you use to create the organization. From the organization's management account, you can do the following:
        Create accounts in the organization
        Invite other existing accounts ot the organization
        Remove accounts from the organization
        Manage invitations
        Apply policies to entities (roots, OUs, or accounts) within the organization
        Enable integration with supported AWS services to provide service functionality across all of the accounts in the organization
    Member accounts make up all of the rest of the accounts in an organization. An account can be a member of only one organization at a time. You can attach a policy to an account to apply controls to only that one account.

Invitation
The process of asking another account to join your organization. An invitation can be issued only by the organiztion's management account. The invitation is extended to either the account ID or the email address that is associated with the invited account. After the invited account accepts an invitation, it becomes a member account in the organization. Invitations also can be sent to all current member accounts when the organization needs all members to approve the change from supporting only consolidating billing features to supporting all features in the organization. Invitations work by accounts exchanging handshakes. You might not see handshakes when you work in the AWS Organizations console. But if you use the AWS CLI or AWS Organizations API, you must work directly with handshakes.

...

Available feature sets
    All features - The default feature set that is available in AWS Organizations. It includes all the functionality of consolidated billing, plus advanced features that give you more control over accounts in your organization. For example, when all features are enabled, the management account of the organization has full control over what member accounts can do. The management account can apply SCPs to restrict the services and actionst hat users (including the root user) and roles in an account can access. The management account can also prevent member accounts form leaving the organization. You can also enable integration with support AWS services to let those services provide fucnctionality across all of the accounts in your organization.

    You can create an organization with all features already enabled, or you can enable all features in an organization that orginallly supported only the consolidated billing features. To enable all features, all invited member accounts must approve the change by accepting the invitation that is sent when the management account starts the process.

    Consolidated billing - This feature set provides shared billing functionality, but does not incldue the more advanced features of AWS Organizations. For example, you can't enable other AWS services to integrate with your organization to work across all of its accounts, or use policies to restrict what users and roles in different accounts can do. To use the advanced AWS Organizations features, you must enable all features in your organization.

Service Control Policy (SCP)
A policy that specifies the services and actions that users and roles can use in teh accounts that the SCP affects. SCPs are similar to IAM permissions policies except that they don't grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account. When you attach an SCP to your organization root or an OU, the SCP limits permissions for entities in the member account.

Allow lists vs deny lists
Aloos lists and deny lists are complementary strategies that you can use to apply SCPs to filter the permissions that are available to accounts

    Allow list strategy -- You explicitly specify the access that is allowed. All other access is implicitly blocked. By default, AWS Organizations attaches an AWS managed policy called FullAWSAccess to all roots, OUs, and accounts. This helps ensure that, as you build your organization, nothing is blocked until you want it to be. In other words, by default all permissions are allowed. HWne you are ready to restrict permissions, you replace the FullAWSAccess policy with one that allows only the more limited, desired set of permissions. Users and roles in the affected accounts can then exercise only that level of access, even if their IAM policies allow all actions. If you replace the default policy on the root, all accounts in the organization are affected by the restrictions. You can't add permissions back at a lower level in hte hierarchy because an SCP never grants permissions; it only filters them.

    Deny list strategy -- You explicitly specify the access that is not allowed. All other access is allowed. In this scenario, all permissions are allowed unless explicitly blocked. This is the default behavior of AWS Organizations. By default, AWS Organizations attaches an AWS managed policy called FullAWSAccess to all roots, OUs, and accounts. THis allwos any account to acces any service or operation with no AWS Organizations-imposed restrictions. Unlike the allow list technique described above, when using deny lists, you leave the default FullAWSAccess policy in place (that allow "all"). But then you attach additional policies that deny acces to the unwanted services and actions. Just as with IAM permission policies, an explicit deny of service action overrides any allow of that action.

Artificial intelligence (AI) services opt-out policy
A type of policy that helps you standardize your opt-out settings for AWS AI services across all accounts in your organization. Certain AWS AI services can store and use customer content processed by those services for the development and continuous improvement of Amazon AI services and technologies. As an AWS customer, you can use AI service opt-out policies to choose to opt out of having your content stored or used for service improvements.

Backup policy
A type of policy that helps you standardize and implement a backup strategy for the resources across all of the accounts in your organization. In a backup policy, you can configure and deploy backup plans for your resources.

...

https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html

Amazon Aurora connection management

Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the hsotname and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don't have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren't available.

For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.

Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements, you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing amoung all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.

Types of Aurora endpoints

An endpoint is represented as an Aurora-specific URL that contains a host address and a port. The following types of endpoints are available form an Aurora DB cluster.

Cluster Endpoint
    A cluster endpoint (or writer endpoint) for an Aurora DB cluster connects to the current primary DB instance for that cluster. This endpoint is the only; one that can perform write operations such as DDL statements. Because of this, the cluster endpoint is the one that you connect to when you first set up a cluster or when your cluster only contains a single DB instance.

    Each Aurora DB cluster has one cluster endpoint and one primary DB instance.

    You use the cluster endpoint for all write operations on the DB cluster, inlcuding inserts, updates, deletes, and DDL changes. You can also use the cluster endpoint for read operations, such as queries.

    The cluster endpoint provides failover support for read/write connections to the DB cluster. If teh current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption to service.

    The following example illustrates a cluster endpoint for an Aurora MySQL DB cluster

    mydbcluster.cluster-123456789012.us-east-1.rds.amazonaws.com:3306

Reader endpoint
    A reader endpoint for an Aurora DB cluster provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint for read operations, such as queries. By processing those statements on the read-only Aurora Replicas, this endpoint reduces the overhead on the primary instance. it also helsp the cluster to scale the capacity to handle simultaneous SELECT queries, proportional to the number of Aurora Replicas in the cluster. Each Aurora DB cluster has one reader endpoint.

    If the cluster contains one or more Aurora Replicas, the reader endpoint load-balances each connection request among the Aurora Replicas. In that case, you can only perform read-only statements such as SELECT in that session. If the cluster only contains a primary instance and no Aurora Replicas, the reader endpoint connects to the primary instance. In that case, you can perform write operations through the endpoint.

    The following example illustrates a reader endpoint for an Aurora MySQL DB cluster.

    mydbcluster.cluster-ro-123456789012.us-east-1.rds.amazonaws.com:3306

Custom endpoints
    A custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection. You define which instances this endpoint refers to, and you decide what purpose the endpoint serves.

    An Aurora DB cluster has no custom endpoints until you create one. You can create up to five custom endpoints for each provisioned Aurora cluster. YOu cant' use custom enpoints for Aurora Serverless clusters.

    The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read/write capability of hte DB instances. For example, you might define a custom endpoint to connect to isntances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances.

    Because the connection can go to any DB instance that is associated with the custom endpoint, we recommend that you make sure that all DB instances within that group share some simialr characteristic. Doing so ensures that the performance, memory capacity, and so on, are consistent for everyone who connects to that endpoint.

    This feature is intended for advanced users with specialized kinds of workloads where it isn't practical to keep all the Aurora Replicas in the cluster identical. With custom endpoints, you can predict the capacity of the DB instance used for each connection. When you use custom endpoints, you typically don't use the reader endpoint for that cluster.

    The following example illustrates a custom endpoint for a DB instance in an Aurora MySQL DB cluster.

    myendpoint.cluster-custom-123456789012.us-east-1.rds.amazonaws.com:3306

Instance endpoint
    An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its own unique instance endpoint. So there is one instance endpoint for the current primary DB instance of the DB cluster, and there is one instance endpoint for each fo the Aurora Replicas in the DB cluster.

    The instance endpoint provides direct control over connections to the DB cluster, for scenarios where using the cluster endpoint or reader endpoint might not be appropriate. For example, your client application might require more fine-grained load balancing absed on workload type. In this case, you can configure multiple clients to connect to different Aurora Replicas in a DB cluster to distribute read workloads. For an example, that uses instance endpoints to improve connection speed after a failover for Aurora PostgreSQL, see Fast failover with Amazon Aurora PostgresSQL. For an example that uses instance endpoints to improve connection speed after a failover for Aurora MySQL, see MarioDB Connector/J failover support - case Amazon Aurora.

    The following example illustrates an instance endpoint for a DB instance in an Aurora MySQL DB cluster.

    mydbinstance.123456789012.us-east-1.rds.amazonaws.com:3306

Viewing the endpoints for an Aurora cluster

In the AWS Management Console, you see the cluster endpoint, the reader endpoint, and any custom endpoints in the detail page for each cluster. You see the instance endpoint in the detail page for each instance. When you connect, you must append the associated port number, following a colon, to the endpoint name shown on this detail page.

With the AWS CLI, you see the writer, reader, and any custom endpoints in teh output of the describe-db-clusters command. For example, the following command shows the endpoint attributes for all clusters in your current AWS Region.

aws rds describe-db-clusters --query '*[].{Endpoint:Endpoint,ReaderEndpoint:ReaderEndpoint,CustomEndpoints:CustomEndpoints}'

With the Amazon RDS API, you retrieve the endpoints by calling the DescribeDBClusterEndpoints function.

Using the cluster endpoint

Because each Aurora cluster has a single built-in cluster endpoint, whose name and other attributes are managed by Aurora, you can't create, delete, or modify this kind of endpoint.

You use the cluster endpoint when you administer your cluster, perform extract, transform, load (ETL) operations, or develop and test applications. The cluster endpoint connects to the primary instance of the cluster. The primary instance is the only DB instance where you can create tables and indexes, run INSERT statements, and perform other DDL and DML operations.

The physical IP address pointed to by the cluster endpoint changes when the failover mechanism promotes a new DB isntance to be the read/write primary instance for the cluster. If you use any form of connection pooling or other multiplexing, be preapred to flush or reduce the time-to-live for any cached DNS information. Doing so ensures that you don't wtry to establish a read/write connection to a DB instance that became unavailable or is now read-only after a failover.

Using the reader endpoint

...

====================
https://aws.amazon.com/blogs/networking-and-content-delivery/secure-hybrid-access-to-amazon-s3-using-aws-privatelink/

Secure hybrid access to Amazon S3 using AWS PrivateLink

AWS PrivateLink for Amazon S3 enables on-premises applications to privately and securely access Amazon S3 over AWS Direct Connect private virtual interface or AWS Site to Site VPN. The Interface VPC Endpoints for Amazon S3 allow security administrators to control which users can access which data in S3 from on premises and cross-Region using their own private IP addresses over a private network.

To privately access Amazon S3 from inside a Amazon Virtual Private Cloud (VPC), you can use Gateway VPC Endpoints for Amazon S3. These allow applications running in a VPC to access S3 without an Internet Gateway or NAT Gateway. When using Gateway VPC Endpoints, VPC endpoint policies are used to restrict access allowing requests to S3 Buckets from a particular VPC. This is the recommended model for accesssing S3 from a VPC in the same region. However, to use a Gateway VPC endpoint from on-premises applications, or to access S3 from a VPC in a different AWS Region, you must set up a fleet of proxy servers with private IP addresses in your VPC. This results in changes to your on-premise applications sot hat they direct requests to the proxy servers, and then forward them to S3 through your VPC endpoint. This comes with administrative overhead, adds the cost of running the proxy servers, and increases the operational complexity of your application.

AWS PrivateLink for Amazon S3 solves these challenges and enables multiple use cases:

    1. Privately accessing S3 from on premises: This feature lets you to allow on-premises users and applications access to S3 buckets, AWS Accounts, or AWS Organizations. Corresponding S3 bucket policies can restrict access from only specific Interface VPC Endpoints.

    2. Accessing s3 from other Regions: allows adminsitrators to use existing private networks for inter-region connectivity (for example, Amazon VPC peering connectiosn of AWS Transit Gateway) while still enforcing VPC, bucket, account, and organizational access policies.

In this blog post, we show how to access Amazon S3 buckets from on-premise networks. For example, AWS DirectConnect or VPN over private connectivity using AWS PrivateLink for S3. We discuss and walk you through how to use various DNS options to enable the onnectivity from on-premise applcations. We also show how to configure your on-premises DNS resolvers to direct S3 domain names to teh interface endpoint IPs by forwarding DNS queries to Amazon Route 53 resolver Inbound Endpoints.

Pre-requisites

Before you begin, make sure you have an Amazon VPC dedicated to this solution. you will also need an AWS DirectConnect connection configured with a private virtual interface (Private VIF), or have AWS-Site-to-Site VPN connection up and running to your VPC.

Create an Interface VPC endpoint for Amazon S3

    1. To create an Inteface VPC endpoint for Amazon S3, first navigate to the VPC console, select Endpoints, and choose Create Endpoint.

    2. For Service Category, ensure that AWS services is selected. Then, filter the service names by entering S3 in the search box. For Service Name, choose the service as "S3" and for Type, ensure that it shows Interface.

    3. Select the VPC, desired AZs, and subnet for each one, and then select the appropriate security groups (this should allow traffic from your networks on port 443) and click on Create endpoint. We show this in the following screenshot (figure 1).

It takes a few moments to go through the interface endpoint lifecycle steps while it is created. After teh interface endpoint is available, you can view its information by choosing Details. The DNS Names field displays the DNS names used to access the service. You will need the DNS names later, so makea note of them now.

Choose subnets to see where the interface endpoint is located, adn the ID of teh endpoint network interface in each subnet. In the following screenshot (figure 3), the private IP address of the endpoint network interface in the VPC are 10.0.1.165 and 10.0.2.19.

DNS considerations for accessing S3 bcukets using AWS PrivateLink

There are a few DNS options to choose form when accessing Amazon S3 over PrivateLink.

Option 1: Access Amazon S3 without modifying your on-premises DNS resolver

AWS Privatelink creates endpoint-specific DNS hostnames that you can use to communicate with the service. These endpoint-specific DNS hostnames resolve to the private IP address of the endpoint network interface in the VPC.

When using endpoint-specific DNS names to access the interface endpoints for Amazon S3, you don't have to update your on-premises DNS resolver. You can resolve the endpoint-specific DNS name to the private IP address of the interface endpoint from the public Amazon S3 DNS hostname, let's say vpce-001760f0be22cd4e2-3ew5rhyv.s3.eu-west-2.vpce.amazonaws.com, from your on-premises applications to access Amazon S3 without modifying your DNS resolver configuration.


https://stackoverflow.com/questions/2998215/if-python-is-interpreted-what-are-pyc-files?rq=1

    I've been given to understand that Python is an interpreted language...

This popular meme is incorrect, or, rather, constructed upon a misunderstanding of (natural) language levels: a similar mistake would be to say "the Bible is a hardcover book". Let me explain that comparison.

"The Bible" is "a book" in the sense of being a class of (actualy, physical objects defined as) books; the books identified as "copies of the Bible" are supposed to have something fundamental in common (the contents, although even those can be in different languages, with different acceptable tranlsations, levels of footnotes, and other annotations) -- however, those books are perfectly well allowed to differ in a myriad of aspects that are NOT considered fundamental -- kind of binding, fonts used in printing, illustrations if any, wide writable margins or not, numbers and kinds fo builtin bookmarks and so on.

It's quite possible that a typical printing of the Bible would indeed be in hardcover binding. After all, it's a book that's typically meant to be read over and over, bookmarked at several places, thumbed through looking for given chapter-and-verse pointeers, etc, and a good hardcover binding can make a given copy last longer for such use. However, these are mundane (practical) issues that cannot be used to determin whether a given actual book is a copy of the Bibele or not: paperback printings are possible.

Similarly, Python is "a language" in the sense of defining a class of language implementations which must all be similar in some fundamental respects (syntax, most semantics except those parts of those where they're explicitly allowed to differ) but are fully allowed to differ in just about every "implementation" detail, including how they deal with the source files they're given, wheter they compile the sources to some lower level forms (and, if so, which form and whether they save such compiled forms, to disk or elsewhere), how they execute said forms, and so forth.

The classical implementation, CPython, is often just called "Python" for short -- but it's just one of several production-quality implementations, side by side with Microsoft's IronPython (which compiles to CLR codes, i.e. ".NET"), Jython (which compiles to JVM codes), PyPy (which is written in Python itself and can compile to a huge variety of "back-end" forms including "just-in-time" generated machine language). They're all Python (== "implementations of the Python language") just like many superficially different book objects can all be Bibles (== "copies of The Bible").

If you're interested in CPython specifically: it compiles the source files into a Python-specific lower-level form (known as "bytecode"), does so automatically when needed (when there is no bytecode file corresponding to a source file, or the bytecode file is older than the source or compiled by a different Python version), usually saves the bytecode files to disk (to avoid recompiling them in the future). OTOH IronPython will typically compile to CLR codes (saving them to disk or not, depending) and Jython to JVM codes (saving them to disk or not -- it will use the .class extension if it does save them).

These lower level form sare then executed as appropriate "virtual machines" also known as "interpreters" -- the CPython VM, the .Net runtime, the Java VM (aka JVM), as appropriate.

So, in this sense (what do typical implementations do), Python is an "interpreted language" if and only if C# and Java are: all of them have a typical implementation strategy of producing bytecode first, then executing it via a VM/interpreter.

More likely the focus is on how "heavy", slow, and high-ceremony the compilation process is. CPython is designed to compile as fast as possible, as lightweight as possible, and with as little ceremony as feasible -- The compile does very little error checking and optimization, so it can run fast and in small amounts of memory, which in turn lets it be run automatically and transparently whenever needed, without the user seven needing to be aware that there is a compliation going on, most of the time. Java and C# typically accept more work during compilation (and therefore don't perform automatic compilation) in order to check errors more thoroughly and perform more optimizations. It's a continuum of grey scales, not a black or white situation, and it would be arbitrary to put a threshold at some given level and say that only above that level can you call it "compilation".



https://aws.amazon.com/blogs/infrastructure-and-automation/deploy-bastion-hosts-into-private-subnets-with-aws-cdk/

Deploy bastion hosts into private subnets with AWS cdk

Deploying bastion hosts in private subnets is a way to provide temporary and limited access to non-production private resources in a virtual private cloud (VPC). Bastion hosts typically sit in public subnets. But in a non-production environment, if you want to allow a group of developers to acces a private resource, you might not want to sue a bastion host accessible from the internet.

In this post, we explain how to provision scalable and extendable secure bastion host in private subnets using AWS Cloud Development Kit (CDK). First, I'll show you how to configure AWS CDK and clone the GitHub repository I've prepared. Second, we demonstrate using AWS CDK to define the target environment and deploy the bastion host stack into a new or existing VPC.

Finally, we create an AWS Identity and Access Management (IAM) policy. We demonstrate how users with that policy can access th bastion host using three operations. We use the AWS Systems Manager Session Manager plugin for the AWS Command Line Interface (AWS CLI), SSH (secure shell), and AWS Mangement Console. Our process follows AWS Security best practices of granting least privilege and using roles to delegate permissions.

































































end of copii

What is the Pick Branch Activity?
AWS related questsions:
    What guardrails do we have in place?
    How does Control Tower unify billing?
    https://aws.amazon.com/blogs/security/automate-analyzing-permissions-using-iam-access-advisor/
